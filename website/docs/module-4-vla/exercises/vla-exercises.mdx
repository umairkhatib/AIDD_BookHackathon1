---
sidebar_position: 1
title: "VLA System Exercises"
---

# Vision-Language-Action System Exercises

## Overview

This section provides hands-on exercises for Vision-Language-Action (VLA) systems. These exercises will help you gain practical experience with integrating Whisper, LLMs, and robotic action execution.

## Exercise 1: Basic Voice Command Processing

### Objective
Create a basic voice command processing system that uses Whisper to convert speech to text and performs simple robot actions.

### Requirements
- Implement a ROS 2 node that captures audio from a microphone
- Use Whisper to transcribe the audio to text
- Map recognized commands to simple robot actions (move forward, turn, etc.)
- Publish appropriate ROS messages to control the robot

### Starter Code
```python
#!/usr/bin/env python3

"""
Exercise 1: Basic Voice Command Processing
Implement a simple voice command system using Whisper
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Twist
import whisper
import pyaudio
import numpy as np
import wave
import tempfile
import os


class BasicVoiceCommandNode(Node):
    def __init__(self):
        super().__init__('basic_voice_command')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.status_pub = self.create_publisher(String, '/voice_status', 10)

        # Initialize Whisper model
        self.model = whisper.load_model("base")  # Use "tiny" for faster processing

        # Audio parameters
        self.audio_format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.chunk = 1024
        self.silence_threshold = 500

        # Initialize audio stream
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(
            format=self.audio_format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )

        # Timer for audio processing
        self.process_timer = self.create_timer(0.1, self.process_audio)

        self.get_logger().info('Basic Voice Command Node initialized')

    def process_audio(self):
        """Process audio from the microphone."""
        try:
            # Read audio chunk
            data = self.stream.read(self.chunk, exception_on_overflow=False)
            audio_array = np.frombuffer(data, dtype=np.int16)

            # Simple voice activity detection
            if np.abs(audio_array).mean() > self.silence_threshold:
                self.get_logger().info('Voice activity detected, recording...')

                # Record a longer segment
                frames = [data]
                for _ in range(0, int(self.rate / self.chunk * 2)):  # Record 2 seconds
                    data = self.stream.read(self.chunk, exception_on_overflow=False)
                    frames.append(data)

                # Convert to WAV and save temporarily
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                    wf = wave.open(temp_file.name, 'wb')
                    wf.setnchannels(self.channels)
                    wf.setsampwidth(self.audio.get_sample_size(self.audio_format))
                    wf.setframerate(self.rate)
                    wf.writeframes(b''.join(frames))
                    wf.close()

                    # Transcribe using Whisper
                    result = self.model.transcribe(temp_file.name)
                    transcribed_text = result["text"].strip()

                    if transcribed_text:
                        self.get_logger().info(f'Transcribed: "{transcribed_text}"')
                        self.execute_command(transcribed_text)

                    # Clean up
                    os.unlink(temp_file.name)

        except Exception as e:
            self.get_logger().error(f'Error processing audio: {e}')

    def execute_command(self, command_text):
        """Execute robot command based on transcribed text."""
        cmd = Twist()

        # Simple command mapping
        if 'forward' in command_text.lower() or 'ahead' in command_text.lower():
            cmd.linear.x = 0.3
            cmd.angular.z = 0.0
        elif 'backward' in command_text.lower() or 'back' in command_text.lower():
            cmd.linear.x = -0.3
            cmd.angular.z = 0.0
        elif 'left' in command_text.lower():
            cmd.linear.x = 0.0
            cmd.angular.z = 0.5
        elif 'right' in command_text.lower():
            cmd.linear.x = 0.0
            cmd.angular.z = -0.5
        elif 'stop' in command_text.lower() or 'halt' in command_text.lower():
            cmd.linear.x = 0.0
            cmd.angular.z = 0.0
        else:
            self.get_logger().info(f'Unknown command: {command_text}')
            return

        # Publish command
        self.cmd_vel_pub.publish(cmd)
        self.get_logger().info(f'Executed command: {command_text}')

    def destroy_node(self):
        """Clean up resources."""
        self.stream.stop_stream()
        self.stream.close()
        self.audio.terminate()
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)

    voice_node = BasicVoiceCommandNode()

    try:
        rclpy.spin(voice_node)
    except KeyboardInterrupt:
        voice_node.get_logger().info('Voice command node stopped by user')
    finally:
        voice_node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Solution Steps
1. Implement the audio capture and Whisper transcription
2. Create command mappings for basic robot movements
3. Test with simple voice commands like "move forward" and "turn left"
4. Verify the robot responds appropriately to commands

### Verification
- Speak simple commands and verify the robot moves accordingly
- Check that transcription accuracy is reasonable
- Ensure the system handles unknown commands gracefully

<details>
<summary>Click for Solution</summary>

The starter code above provides a complete implementation of the basic voice command system. The key components are:

1. **Audio Capture**: Uses PyAudio to capture microphone input
2. **Voice Activity Detection**: Simple threshold-based detection
3. **Whisper Integration**: Transcribes captured audio to text
4. **Command Mapping**: Maps text commands to robot actions
5. **Action Execution**: Publishes Twist messages to control the robot

The implementation handles basic movement commands and includes proper resource cleanup.

</details>

## Exercise 2: Context-Aware Voice Commands

### Objective
Extend the basic system to incorporate environmental context when interpreting commands.

### Requirements
- Subscribe to sensor data (LIDAR, camera, etc.)
- Use environmental context to disambiguate commands
- Implement more sophisticated command interpretation
- Handle spatial references in commands (e.g., "go to the left door")

### Starter Code
```python
#!/usr/bin/env python3

"""
Exercise 2: Context-Aware Voice Commands
Implement voice commands that consider environmental context
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import LaserScan, Image
from geometry_msgs.msg import Twist, PoseStamped
from nav_msgs.msg import OccupancyGrid
from cv_bridge import CvBridge
import whisper
import pyaudio
import numpy as np
import wave
import tempfile
import os
import json


class ContextAwareVoiceNode(Node):
    def __init__(self):
        super().__init__('context_aware_voice')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.nav_goal_pub = self.create_publisher(PoseStamped, '/goal_pose', 10)
        self.status_pub = self.create_publisher(String, '/voice_status', 10)

        # Subscribers
        self.scan_sub = self.create_subscription(
            LaserScan, '/scan', self.scan_callback, 10
        )
        self.camera_sub = self.create_subscription(
            Image, '/camera/rgb/image_raw', self.camera_callback, 10
        )
        self.map_sub = self.create_subscription(
            OccupancyGrid, '/map', self.map_callback, 10
        )

        # Initialize Whisper model
        self.model = whisper.load_model("base")

        # Audio parameters
        self.audio_format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.chunk = 1024
        self.silence_threshold = 500

        # Initialize audio stream
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(
            format=self.audio_format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )

        # Context variables
        self.laser_data = None
        self.map_data = None
        self.environment_context = {
            'obstacles': [],
            'free_spaces': [],
            'landmarks': [],
            'robot_pose': None
        }

        # CV Bridge for image processing
        self.cv_bridge = CvBridge()

        # Timer for audio processing
        self.process_timer = self.create_timer(0.1, self.process_audio_with_context)

        self.get_logger().info('Context-Aware Voice Command Node initialized')

    def scan_callback(self, msg):
        """Process LIDAR data for context."""
        self.laser_data = msg

        # Analyze obstacles in the environment
        if msg.ranges:
            obstacles = []
            free_spaces = []

            for i, range_val in enumerate(msg.ranges):
                if 0 < range_val < 2.0:  # Obstacles within 2m
                    angle = msg.angle_min + i * msg.angle_increment
                    obstacles.append({
                        'distance': range_val,
                        'angle': angle,
                        'x': range_val * np.cos(angle),
                        'y': range_val * np.sin(angle)
                    })
                elif range_val > 3.0:  # Free space beyond 3m
                    angle = msg.angle_min + i * msg.angle_increment
                    free_spaces.append({
                        'distance': range_val,
                        'angle': angle
                    })

            self.environment_context['obstacles'] = obstacles
            self.environment_context['free_spaces'] = free_spaces

    def camera_callback(self, msg):
        """Process camera data for context."""
        try:
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")

            # In a real implementation, you would use object detection
            # to identify landmarks and features in the environment
            # For this example, we'll just store that we have camera data
            self.environment_context['has_camera_data'] = True

        except Exception as e:
            self.get_logger().error(f'Error processing camera data: {e}')

    def map_callback(self, msg):
        """Process map data for context."""
        self.map_data = msg
        self.environment_context['map_info'] = {
            'resolution': msg.info.resolution,
            'origin': (msg.info.origin.position.x, msg.info.origin.position.y)
        }

    def process_audio_with_context(self):
        """Process audio with environmental context."""
        try:
            # Check for voice activity
            data = self.stream.read(self.chunk, exception_on_overflow=False)
            audio_array = np.frombuffer(data, dtype=np.int16)

            if np.abs(audio_array).mean() > self.silence_threshold:
                self.get_logger().info('Voice activity detected, recording with context...')

                # Record a longer segment
                frames = [data]
                for _ in range(0, int(self.rate / self.chunk * 2)):
                    data = self.stream.read(self.chunk, exception_on_overflow=False)
                    frames.append(data)

                # Convert to WAV and save temporarily
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                    wf = wave.open(temp_file.name, 'wb')
                    wf.setnchannels(self.channels)
                    wf.setsampwidth(self.audio.get_sample_size(self.audio_format))
                    wf.setframerate(self.rate)
                    wf.writeframes(b''.join(frames))
                    wf.close()

                    # Transcribe using Whisper
                    result = self.model.transcribe(temp_file.name)
                    transcribed_text = result["text"].strip()

                    if transcribed_text:
                        self.get_logger().info(f'Transcribed: "{transcribed_text}"')

                        # Interpret with context
                        self.interpret_command_with_context(transcribed_text)

                    # Clean up
                    os.unlink(temp_file.name)

        except Exception as e:
            self.get_logger().error(f'Error processing audio with context: {e}')

    def interpret_command_with_context(self, command_text):
        """Interpret command using environmental context."""
        command_lower = command_text.lower()

        # Use context to disambiguate spatial commands
        if 'left' in command_lower or 'right' in command_lower:
            # Check if there are obstacles/objects to the left/right
            obstacles = self.environment_context['obstacles']

            if 'go to the left' in command_lower:
                # Find free space to the left
                left_free = [obs for obs in obstacles if -1.57 < obs['angle'] < -0.17]  # Approximately left
                if not left_free:
                    self.get_logger().info('Going left - path appears clear')
                    cmd = Twist()
                    cmd.linear.x = 0.3
                    cmd.angular.z = -0.2  # Slight left turn
                    self.cmd_vel_pub.publish(cmd)
                else:
                    self.get_logger().warn('Left path blocked by obstacles')

            elif 'go to the right' in command_lower:
                # Find free space to the right
                right_free = [obs for obs in obstacles if 0.17 < obs['angle'] < 1.57]  # Approximately right
                if not right_free:
                    self.get_logger().info('Going right - path appears clear')
                    cmd = Twist()
                    cmd.linear.x = 0.3
                    cmd.angular.z = 0.2  # Slight right turn
                    self.cmd_vel_pub.publish(cmd)
                else:
                    self.get_logger().warn('Right path blocked by obstacles')

        elif 'forward' in command_lower:
            # Check if path is clear ahead
            front_obstacles = [obs for obs in obstacles if -0.78 < obs['angle'] < 0.78]  # Front 90 degrees
            if not front_obstacles or min([obs['distance'] for obs in front_obstacles]) > 1.0:
                cmd = Twist()
                cmd.linear.x = 0.3
                cmd.angular.z = 0.0
                self.cmd_vel_pub.publish(cmd)
                self.get_logger().info('Moving forward - path clear')
            else:
                closest = min(front_obstacles, key=lambda x: x['distance'])
                self.get_logger().warn(f'Forward path blocked at {closest["distance"]:.2f}m')

        elif 'stop' in command_lower or 'halt' in command_lower:
            cmd = Twist()
            cmd.linear.x = 0.0
            cmd.angular.z = 0.0
            self.cmd_vel_pub.publish(cmd)
            self.get_logger().info('Stopping robot')

        else:
            self.get_logger().info(f'Command not handled with context: {command_text}')

    def destroy_node(self):
        """Clean up resources."""
        self.stream.stop_stream()
        self.stream.close()
        self.audio.terminate()
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)

    context_node = ContextAwareVoiceNode()

    try:
        rclpy.spin(context_node)
    except KeyboardInterrupt:
        context_node.get_logger().info('Context-aware voice node stopped by user')
    finally:
        context_node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Solution Steps
1. Implement sensor data subscribers (LIDAR, camera, map)
2. Process sensor data to build environmental context
3. Extend command interpretation to consider spatial relationships
4. Test commands that reference environmental features

### Verification
- Test spatial commands like "go left" and verify they consider obstacles
- Confirm the robot avoids obstacles when commanded to move in that direction
- Verify that context-aware commands work differently than basic commands

<details>
<summary>Click for Solution</summary>

The solution implements context awareness by:

1. **Environmental Sensing**: Subscribing to LIDAR, camera, and map data
2. **Context Building**: Analyzing sensor data to identify obstacles, free spaces, and landmarks
3. **Spatial Reasoning**: Using angle and distance information to interpret directional commands
4. **Safe Execution**: Checking for obstacles before executing movement commands

The system enhances basic voice commands with environmental awareness, making the robot safer and more capable of following spatial instructions accurately.

</details>

## Exercise 3: LLM-Powered Command Interpretation

### Objective
Integrate a Large Language Model (LLM) to interpret complex voice commands and convert them to action sequences.

### Requirements
- Integrate with an LLM API (OpenAI GPT, HuggingFace, or local model)
- Convert natural language commands to structured robot actions
- Handle complex, multi-step commands
- Implement error handling and safety checks

### Starter Code
```python
#!/usr/bin/env python3

"""
Exercise 3: LLM-Powered Command Interpretation
Implement voice commands with LLM-powered interpretation
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Twist, PoseStamped
from action_msgs.msg import GoalStatus
import whisper
import pyaudio
import numpy as np
import wave
import tempfile
import os
import json
import time
from typing import Dict, List, Any


class LLMVoiceCommandNode(Node):
    def __init__(self):
        super().__init__('llm_voice_command')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.nav_goal_pub = self.create_publisher(PoseStamped, '/goal_pose', 10)
        self.status_pub = self.create_publisher(String, '/voice_status', 10)

        # Initialize Whisper model
        self.model = whisper.load_model("base")

        # Audio parameters
        self.audio_format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.chunk = 1024
        self.silence_threshold = 500

        # Initialize audio stream
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(
            format=self.audio_format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )

        # Robot capabilities
        self.robot_capabilities = {
            "navigation": ["move_forward", "move_backward", "turn_left", "turn_right", "go_to_location"],
            "manipulation": ["pick_up", "place_down", "grasp", "release"],
            "interaction": ["speak", "listen", "follow", "wait"]
        }

        # Known locations
        self.known_locations = {
            "kitchen": {"x": 5.0, "y": 3.0, "theta": 0.0},
            "living_room": {"x": 0.0, "y": 0.0, "theta": 0.0},
            "bedroom": {"x": -3.0, "y": 2.0, "theta": 1.57},
            "office": {"x": 2.0, "y": -2.0, "theta": 3.14}
        }

        # Timer for audio processing
        self.process_timer = self.create_timer(0.1, self.process_audio_with_llm)

        self.get_logger().info('LLM Voice Command Node initialized')

    def process_audio_with_llm(self):
        """Process audio and interpret with LLM."""
        try:
            # Check for voice activity
            data = self.stream.read(self.chunk, exception_on_overflow=False)
            audio_array = np.frombuffer(data, dtype=np.int16)

            if np.abs(audio_array).mean() > self.silence_threshold:
                self.get_logger().info('Voice activity detected, processing with LLM...')

                # Record a longer segment
                frames = [data]
                for _ in range(0, int(self.rate / self.chunk * 3)):  # Record 3 seconds
                    data = self.stream.read(self.chunk, exception_on_overflow=False)
                    frames.append(data)

                # Convert to WAV and save temporarily
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                    wf = wave.open(temp_file.name, 'wb')
                    wf.setnchannels(self.channels)
                    wf.setsampwidth(self.audio.get_sample_size(self.audio_format))
                    wf.setframerate(self.rate)
                    wf.writeframes(b''.join(frames))
                    wf.close()

                    # Transcribe using Whisper
                    result = self.model.transcribe(temp_file.name)
                    transcribed_text = result["text"].strip()

                    if transcribed_text:
                        self.get_logger().info(f'Transcribed: "{transcribed_text}"')

                        # Interpret with LLM
                        action_sequence = self.interpret_with_llm(transcribed_text)
                        if action_sequence:
                            self.execute_action_sequence(action_sequence)

                    # Clean up
                    os.unlink(temp_file.name)

        except Exception as e:
            self.get_logger().error(f'Error processing audio with LLM: {e}')

    def interpret_with_llm(self, command_text: str) -> List[Dict[str, Any]]:
        """
        Interpret command using LLM and return action sequence.
        In a real implementation, this would call an LLM API.
        For this example, we'll simulate the LLM response.
        """
        self.get_logger().info(f'Interpreting command with LLM: {command_text}')

        # Simulated LLM interpretation (in real implementation, call actual LLM)
        command_lower = command_text.lower()

        # Simple rule-based mapping for demonstration
        # In real implementation, use an actual LLM API
        if 'bring me coffee' in command_lower:
            return [
                {"action": "navigate", "location": "kitchen"},
                {"action": "locate", "object": "coffee"},
                {"action": "grasp", "object": "coffee"},
                {"action": "navigate", "location": "living_room"},
                {"action": "deliver", "recipient": "user"}
            ]
        elif 'go to kitchen' in command_lower:
            return [
                {"action": "navigate", "location": "kitchen"}
            ]
        elif 'move forward' in command_lower or 'go ahead' in command_lower:
            return [
                {"action": "move", "direction": "forward", "distance": 1.0}
            ]
        elif 'turn left' in command_lower:
            return [
                {"action": "turn", "direction": "left", "angle": 90}
            ]
        elif 'turn right' in command_lower:
            return [
                {"action": "turn", "direction": "right", "angle": 90}
            ]
        elif 'stop' in command_lower:
            return [
                {"action": "stop"}
            ]
        elif 'follow me' in command_lower:
            return [
                {"action": "follow", "target": "user"}
            ]
        else:
            # For unknown commands, try to extract intent
            if any(word in command_lower for word in ['go', 'navigate', 'move', 'travel']):
                # Look for location in known locations
                for location in self.known_locations:
                    if location in command_lower:
                        return [
                            {"action": "navigate", "location": location}
                        ]

            self.get_logger().info(f'Could not interpret command: {command_text}')
            return []

    def execute_action_sequence(self, action_sequence: List[Dict[str, Any]]):
        """Execute a sequence of actions."""
        self.get_logger().info(f'Executing action sequence: {action_sequence}')

        for action in action_sequence:
            action_type = action.get("action")

            if action_type == "navigate":
                location = action.get("location")
                if location in self.known_locations:
                    loc_data = self.known_locations[location]
                    self.navigate_to_location(loc_data)
                else:
                    self.get_logger().warn(f'Unknown location: {location}')

            elif action_type == "move":
                direction = action.get("direction")
                distance = action.get("distance", 1.0)
                self.move_direction(direction, distance)

            elif action_type == "turn":
                direction = action.get("direction")
                angle = action.get("angle", 90)
                self.turn_direction(direction, angle)

            elif action_type == "stop":
                self.stop_robot()

            elif action_type == "follow":
                target = action.get("target")
                self.follow_target(target)

            # Add delay between actions
            time.sleep(0.5)

    def navigate_to_location(self, location_data: Dict[str, float]):
        """Navigate to a specific location."""
        goal_msg = PoseStamped()
        goal_msg.header.stamp = self.get_clock().now().to_msg()
        goal_msg.header.frame_id = 'map'

        goal_msg.pose.position.x = location_data['x']
        goal_msg.pose.position.y = location_data['y']
        goal_msg.pose.position.z = 0.0

        # Convert angle to quaternion
        import math
        theta = location_data['theta']
        goal_msg.pose.orientation.z = math.sin(theta / 2.0)
        goal_msg.pose.orientation.w = math.cos(theta / 2.0)

        self.nav_goal_pub.publish(goal_msg)
        self.get_logger().info(f'Navigating to location: {location_data}')

    def move_direction(self, direction: str, distance: float):
        """Move in a specific direction."""
        cmd = Twist()

        if direction == "forward":
            cmd.linear.x = 0.3
        elif direction == "backward":
            cmd.linear.x = -0.3
        elif direction == "left":
            cmd.angular.z = 0.5
        elif direction == "right":
            cmd.angular.z = -0.5
        else:
            self.get_logger().warn(f'Unknown direction: {direction}')
            return

        # Publish command for specified duration
        duration = distance / 0.3  # Assuming 0.3 m/s speed
        start_time = time.time()

        while time.time() - start_time < duration and rclpy.ok():
            self.cmd_vel_pub.publish(cmd)
            time.sleep(0.05)

        # Stop robot
        self.stop_robot()

    def turn_direction(self, direction: str, angle_degrees: float):
        """Turn in a specific direction."""
        cmd = Twist()
        angular_speed = 0.5  # rad/s
        angle_radians = math.radians(angle_degrees)
        duration = angle_radians / angular_speed

        if direction == "left":
            cmd.angular.z = angular_speed
        elif direction == "right":
            cmd.angular.z = -angular_speed
        else:
            self.get_logger().warn(f'Unknown turn direction: {direction}')
            return

        start_time = time.time()
        while time.time() - start_time < duration and rclpy.ok():
            self.cmd_vel_pub.publish(cmd)
            time.sleep(0.05)

        # Stop robot
        self.stop_robot()

    def stop_robot(self):
        """Stop robot movement."""
        cmd = Twist()
        cmd.linear.x = 0.0
        cmd.angular.z = 0.0
        self.cmd_vel_pub.publish(cmd)
        self.get_logger().info('Robot stopped')

    def follow_target(self, target: str):
        """Follow a target (simplified implementation)."""
        self.get_logger().info(f'Following target: {target}')
        # In a real implementation, this would use person detection/tracking

        # For now, just move forward
        cmd = Twist()
        cmd.linear.x = 0.2
        cmd.angular.z = 0.0
        self.cmd_vel_pub.publish(cmd)

    def destroy_node(self):
        """Clean up resources."""
        self.stream.stop_stream()
        self.stream.close()
        self.audio.terminate()
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)

    llm_node = LLMVoiceCommandNode()

    try:
        rclpy.spin(llm_node)
    except KeyboardInterrupt:
        llm_node.get_logger().info('LLM voice command node stopped by user')
    finally:
        llm_node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Solution Steps
1. Implement LLM interface for command interpretation
2. Create action sequence generation from natural language
3. Implement action sequence execution
4. Add safety checks and error handling

### Verification
- Test complex commands like "bring me coffee" and verify they break down into steps
- Confirm the robot can handle multi-step commands
- Verify error handling for unrecognized commands

<details>
<summary>Click for Solution</summary>

The LLM-powered command interpretation solution demonstrates:

1. **Natural Language Understanding**: Converting natural language to structured actions
2. **Action Sequencing**: Breaking down complex commands into sequences
3. **Knowledge Integration**: Using known locations and capabilities
4. **Safe Execution**: Implementing proper error handling and safety checks

The system bridges the gap between human-friendly natural language and robot-executable actions.

</details>

## Exercise 4: VLA Integration with Simulation

### Objective
Integrate the VLA system with Isaac Sim for end-to-end testing in a simulated environment.

### Requirements
- Connect the VLA system to Isaac Sim
- Process voice commands in the simulation environment
- Execute actions in the simulated world
- Demonstrate complete VLA pipeline

### Starter Code
```python
#!/usr/bin/env python3

"""
Exercise 4: VLA Integration with Simulation
Integrate the VLA system with Isaac Sim for simulation testing
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Twist, PoseStamped
from sensor_msgs.msg import LaserScan, Image
from cv_bridge import CvBridge
import whisper
import pyaudio
import numpy as np
import wave
import tempfile
import os
import time


class VLASimulationNode(Node):
    def __init__(self):
        super().__init__('vla_simulation_node')

        # Publishers for simulation
        self.cmd_vel_pub = self.create_publisher(Twist, '/robot/cmd_vel', 10)
        self.nav_goal_pub = self.create_publisher(PoseStamped, '/goal_pose', 10)
        self.status_pub = self.create_publisher(String, '/vla_status', 10)

        # Subscribers for simulation sensors
        self.scan_sub = self.create_subscription(
            LaserScan, '/scan', self.scan_callback, 10
        )
        self.camera_sub = self.create_subscription(
            Image, '/rgb_camera/image', self.camera_callback, 10
        )

        # Initialize Whisper model
        self.model = whisper.load_model("base")

        # Audio parameters
        self.audio_format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.chunk = 1024
        self.silence_threshold = 500

        # Initialize audio stream
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(
            format=self.audio_format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )

        # Context variables
        self.laser_data = None
        self.camera_data = None
        self.environment_context = {
            'obstacles': [],
            'free_spaces': [],
            'landmarks': [],
            'robot_pose': None
        }

        # CV Bridge for image processing
        self.cv_bridge = CvBridge()

        # Timer for processing
        self.process_timer = self.create_timer(0.1, self.vla_processing_loop)

        self.get_logger().info('VLA Simulation Node initialized')

    def scan_callback(self, msg):
        """Process LIDAR data from simulation."""
        self.laser_data = msg

        # Update environment context with obstacles
        if msg.ranges:
            obstacles = []
            for i, range_val in enumerate(msg.ranges):
                if 0 < range_val < 3.0:  # Obstacles within 3m
                    angle = msg.angle_min + i * msg.angle_increment
                    obstacles.append({
                        'distance': range_val,
                        'angle': angle
                    })
            self.environment_context['obstacles'] = obstacles

    def camera_callback(self, msg):
        """Process camera data from simulation."""
        try:
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")
            self.camera_data = cv_image
            self.environment_context['has_camera'] = True
        except Exception as e:
            self.get_logger().error(f'Error processing camera data: {e}')

    def vla_processing_loop(self):
        """Main VLA processing loop."""
        try:
            # Check for voice activity
            data = self.stream.read(self.chunk, exception_on_overflow=False)
            audio_array = np.frombuffer(data, dtype=np.int16)

            if np.abs(audio_array).mean() > self.silence_threshold:
                self.get_logger().info('Voice command detected in simulation')

                # Record a segment
                frames = [data]
                for _ in range(0, int(self.rate / self.chunk * 3)):
                    data = self.stream.read(self.chunk, exception_on_overflow=False)
                    frames.append(data)

                # Process with Whisper
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                    wf = wave.open(temp_file.name, 'wb')
                    wf.setnchannels(self.channels)
                    wf.setsampwidth(self.audio.get_sample_size(self.audio_format))
                    wf.setframerate(self.rate)
                    wf.writeframes(b''.join(frames))
                    wf.close()

                    # Transcribe
                    result = self.model.transcribe(temp_file.name)
                    command_text = result["text"].strip()

                    if command_text:
                        self.get_logger().info(f'Simulated command: "{command_text}"')

                        # Process with context from simulation
                        self.process_command_with_simulation_context(command_text)

                    os.unlink(temp_file.name)

        except Exception as e:
            self.get_logger().error(f'Error in VLA processing loop: {e}')

    def process_command_with_simulation_context(self, command_text):
        """Process command considering simulation context."""
        self.get_logger().info(f'Processing: {command_text} with context: {self.environment_context}')

        # Example: Use context to make decisions
        command_lower = command_text.lower()

        if 'move' in command_lower or 'go' in command_lower:
            # Check if path is clear based on LIDAR data
            obstacles = self.environment_context.get('obstacles', [])

            if obstacles:
                closest_obstacle = min(obstacles, key=lambda x: x['distance'])
                if closest_obstacle['distance'] < 0.8:  # Too close to obstacle
                    self.get_logger().warn(f'Obstacle detected at {closest_obstacle["distance"]:.2f}m, not moving')
                    status_msg = String()
                    status_msg.data = f"OBSTACLE_AT_{closest_obstacle['distance']:.2f}m"
                    self.status_pub.publish(status_msg)
                    return

            # Execute movement if path is clear
            if 'forward' in command_lower or 'ahead' in command_lower:
                cmd = Twist()
                cmd.linear.x = 0.3
                cmd.angular.z = 0.0
                self.cmd_vel_pub.publish(cmd)
            elif 'left' in command_lower:
                cmd = Twist()
                cmd.linear.x = 0.1
                cmd.angular.z = 0.5
                self.cmd_vel_pub.publish(cmd)
            elif 'right' in command_lower:
                cmd = Twist()
                cmd.linear.x = 0.1
                cmd.angular.z = -0.5
                self.cmd_vel_pub.publish(cmd)

        elif 'stop' in command_lower:
            cmd = Twist()
            cmd.linear.x = 0.0
            cmd.angular.z = 0.0
            self.cmd_vel_pub.publish(cmd)

        # Publish status update
        status_msg = String()
        status_msg.data = f"EXECUTED:{command_text}|CONTEXT:obstacles_{len(obstacles)}"
        self.status_pub.publish(status_msg)

    def destroy_node(self):
        """Clean up resources."""
        self.stream.stop_stream()
        self.stream.close()
        self.audio.terminate()
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)

    vla_sim_node = VLASimulationNode()

    try:
        rclpy.spin(vla_sim_node)
    except KeyboardInterrupt:
        vla_sim_node.get_logger().info('VLA simulation node stopped by user')
    finally:
        vla_sim_node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Solution Steps
1. Integrate with Isaac Sim sensor data
2. Process voice commands in simulation context
3. Execute actions in simulated environment
4. Test complete VLA pipeline

### Verification
- Run the VLA system in Isaac Sim
- Give voice commands and verify the simulated robot responds
- Confirm context-aware behaviors work in simulation
- Test the complete pipeline from voice input to action execution

<details>
<summary>Click for Solution</summary>

The simulation integration solution shows how to:

1. **Connect to Simulation**: Subscribe to simulated sensor data
2. **Context Awareness**: Use simulation data for context-aware commands
3. **Action Execution**: Publish commands to simulated robot
4. **Complete Pipeline**: Demonstrate end-to-end VLA functionality

This provides a safe environment to test VLA systems before deploying on real hardware.

</details>

## Exercise 5: Advanced VLA Capabilities

### Objective
Implement advanced VLA capabilities including memory, learning, and adaptation.

### Requirements
- Implement command history and context memory
- Add learning from user feedback
- Create adaptive behavior based on environment
- Implement error recovery and graceful degradation

### Starter Code
```python
#!/usr/bin/env python3

"""
Exercise 5: Advanced VLA Capabilities
Implement advanced VLA features including memory, learning, and adaptation
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Twist, PoseStamped
from sensor_msgs.msg import LaserScan
import whisper
import pyaudio
import numpy as np
import wave
import tempfile
import os
import json
import time
from collections import deque
from typing import Dict, List, Any, Optional


class AdvancedVLANode(Node):
    def __init__(self):
        super().__init__('advanced_vla_node')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.nav_goal_pub = self.create_publisher(PoseStamped, '/goal_pose', 10)
        self.status_pub = self.create_publisher(String, '/advanced_vla_status', 10)

        # Subscribers
        self.scan_sub = self.create_subscription(
            LaserScan, '/scan', self.scan_callback, 10
        )

        # Initialize Whisper model
        self.model = whisper.load_model("base")

        # Audio parameters
        self.audio_format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.chunk = 1024
        self.silence_threshold = 500

        # Initialize audio stream
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(
            format=self.audio_format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )

        # Advanced VLA features
        self.command_history = deque(maxlen=20)  # Keep last 20 commands
        self.user_preferences = {}  # Store user preferences
        self.environment_memory = {}  # Remember environment features
        self.error_log = deque(maxlen=50)  # Track errors for learning
        self.adaptation_rules = {}  # Learned adaptation rules

        # Context variables
        self.laser_data = None
        self.last_action_time = time.time()
        self.action_success_rate = 0.0

        # Timer for advanced processing
        self.process_timer = self.create_timer(0.1, self.advanced_vla_loop)
        self.learning_timer = self.create_timer(5.0, self.learning_update)

        self.get_logger().info('Advanced VLA Node initialized with memory and learning')

    def scan_callback(self, msg):
        """Process LIDAR data."""
        self.laser_data = msg

    def advanced_vla_loop(self):
        """Advanced VLA processing loop with memory and adaptation."""
        try:
            # Check for voice activity
            data = self.stream.read(self.chunk, exception_on_overflow=False)
            audio_array = np.frombuffer(data, dtype=np.int16)

            if np.abs(audio_array).mean() > self.silence_threshold:
                self.get_logger().info('Voice activity detected, processing with advanced VLA...')

                # Record segment
                frames = [data]
                for _ in range(0, int(self.rate / self.chunk * 3)):
                    data = self.stream.read(self.chunk, exception_on_overflow=False)
                    frames.append(data)

                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                    wf = wave.open(temp_file.name, 'wb')
                    wf.setnchannels(self.channels)
                    wf.setsampwidth(self.audio.get_sample_size(self.audio_format))
                    wf.setframerate(self.rate)
                    wf.writeframes(b''.join(frames))
                    wf.close()

                    # Transcribe
                    result = self.model.transcribe(temp_file.name)
                    command_text = result["text"].strip()

                    if command_text:
                        self.get_logger().info(f'Advanced processing: "{command_text}"')

                        # Process with advanced features
                        self.process_command_advanced(command_text)

                    os.unlink(temp_file.name)

        except Exception as e:
            self.get_logger().error(f'Error in advanced VLA loop: {e}')

    def process_command_advanced(self, command_text: str):
        """Process command with advanced features."""
        start_time = time.time()

        # Add to command history
        command_entry = {
            'timestamp': start_time,
            'command': command_text,
            'context': self.get_environment_context()
        }
        self.command_history.append(command_entry)

        # Apply learned adaptations
        adapted_command = self.apply_adaptations(command_text)

        # Execute command
        success = self.execute_adapted_command(adapted_command)

        # Log execution result
        execution_record = {
            'command': command_text,
            'adapted_command': adapted_command,
            'success': success,
            'execution_time': time.time() - start_time,
            'timestamp': time.time()
        }
        self.error_log.append(execution_record)

        # Update success rate
        successful_executions = [r for r in list(self.error_log)[-10:] if r['success']]
        self.action_success_rate = len(successful_executions) / min(len(self.error_log), 10)

        # Publish status
        status_msg = String()
        status_msg.data = f"ADVANCED_PROCESSING:cmd='{command_text}'|success_rate={self.action_success_rate:.2f}|mem_size={len(self.command_history)}"
        self.status_pub.publish(status_msg)

    def get_environment_context(self) -> Dict[str, Any]:
        """Get current environment context."""
        context = {
            'timestamp': time.time(),
            'obstacles_count': len(self.environment_context.get('obstacles', [])),
            'last_command_time': self.last_action_time,
            'robot_pose': self.environment_context.get('robot_pose'),
            'battery_level': self.environment_context.get('battery_level', 100.0)
        }
        return context

    def apply_adaptations(self, original_command: str) -> str:
        """Apply learned adaptations to the command."""
        adapted_command = original_command.lower()

        # Apply learned rules based on context and history
        for condition, adaptation in self.adaptation_rules.items():
            if condition in adapted_command:
                # Apply adaptation based on learned patterns
                if 'slow' in condition and self.action_success_rate < 0.7:
                    # If success rate is low, make movements more cautious
                    if 'move' in adapted_command:
                        adapted_command = adapted_command.replace('move', 'carefully move')
                elif 'fast' in condition and self.action_success_rate > 0.9:
                    # If success rate is high, can be more confident
                    if 'navigate' in adapted_command:
                        adapted_command = adapted_command.replace('navigate', 'quickly navigate')

        return adapted_command

    def execute_adapted_command(self, adapted_command: str) -> bool:
        """Execute the adapted command and return success status."""
        try:
            # Simple command mapping for demonstration
            if 'carefully move forward' in adapted_command:
                # Execute more cautious movement
                cmd = Twist()
                cmd.linear.x = 0.1  # Slower
                self.cmd_vel_pub.publish(cmd)
                time.sleep(1.0)
                self.stop_robot()
            elif 'move forward' in adapted_command:
                cmd = Twist()
                cmd.linear.x = 0.3
                self.cmd_vel_pub.publish(cmd)
                time.sleep(1.0)
                self.stop_robot()
            elif 'stop' in adapted_command or 'halt' in adapted_command:
                self.stop_robot()
            else:
                # For unknown commands, try to extract basic intent
                if 'move' in adapted_command:
                    cmd = Twist()
                    cmd.linear.x = 0.2
                    self.cmd_vel_pub.publish(cmd)
                    time.sleep(0.5)
                    self.stop_robot()

            self.last_action_time = time.time()
            return True

        except Exception as e:
            self.get_logger().error(f'Error executing adapted command: {e}')
            return False

    def learning_update(self):
        """Periodically update learning and adaptation rules."""
        if len(self.error_log) < 5:
            return

        # Analyze recent execution patterns
        recent_executions = list(self.error_log)[-10:]

        # Identify patterns in failures
        failed_commands = [entry for entry in recent_executions if not entry['success']]
        successful_commands = [entry for entry in recent_executions if entry['success']]

        # Update adaptation rules based on patterns
        if len(failed_commands) > len(successful_commands) * 0.3:  # More than 30% failure rate
            # Be more cautious
            if 'cautious_movement' not in self.adaptation_rules:
                self.adaptation_rules['cautious_movement'] = {
                    'condition': 'move',
                    'adaptation': 'apply_cautious_parameters'
                }
                self.get_logger().info('Learning: Increased caution due to failures')

        elif self.action_success_rate > 0.9:
            # Be more confident
            self.adaptation_rules.pop('cautious_movement', None)
            self.get_logger().info('Learning: Increased confidence due to successes')

        # Update user preferences based on command patterns
        command_texts = [entry['command'] for entry in recent_executions]
        self.update_user_preferences(command_texts)

    def update_user_preferences(self, command_texts: List[str]):
        """Update user preferences based on command patterns."""
        # Analyze command patterns to infer user preferences
        for text in command_texts:
            if 'slow' in text.lower() or 'careful' in text.lower():
                self.user_preferences['movement_style'] = 'cautious'
            elif 'fast' in text.lower() or 'quick' in text.lower():
                self.user_preferences['movement_style'] = 'efficient'

    def stop_robot(self):
        """Stop robot movement."""
        cmd = Twist()
        cmd.linear.x = 0.0
        cmd.angular.z = 0.0
        self.cmd_vel_pub.publish(cmd)

    def get_system_state(self) -> Dict[str, Any]:
        """Get current system state for debugging/monitoring."""
        return {
            'command_history_size': len(self.command_history),
            'error_log_size': len(self.error_log),
            'adaptation_rules_count': len(self.adaptation_rules),
            'success_rate': self.action_success_rate,
            'user_preferences': self.user_preferences,
            'last_action_time': self.last_action_time
        }

    def destroy_node(self):
        """Clean up resources."""
        self.stream.stop_stream()
        self.stream.close()
        self.audio.terminate()
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)

    advanced_vla_node = AdvancedVLANode()

    try:
        rclpy.spin(advanced_vla_node)
    except KeyboardInterrupt:
        advanced_vla_node.get_logger().info('Advanced VLA node stopped by user')

        # Print system state summary
        state = advanced_vla_node.get_system_state()
        advanced_vla_node.get_logger().info(f'Final system state: {json.dumps(state, indent=2)}')

    finally:
        advanced_vla_node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Solution Steps
1. Implement command history and context memory
2. Add learning from execution outcomes
3. Create adaptation rules based on patterns
4. Implement user preference learning

### Verification
- Test that the system remembers past commands
- Verify that it adapts behavior based on success/failure patterns
- Confirm that user preferences are learned over time
- Ensure error recovery mechanisms work properly

<details>
<summary>Click for Solution</summary>

The advanced VLA capabilities solution demonstrates:

1. **Memory Systems**: Keeping track of command history and environmental context
2. **Learning Mechanisms**: Adapting behavior based on execution outcomes
3. **User Modeling**: Learning and adapting to user preferences
4. **Adaptive Behavior**: Modifying actions based on learned patterns

This represents a more sophisticated VLA system that can improve its performance over time and adapt to user preferences and environmental conditions.

</details>

## Assessment Rubric

### Performance Evaluation

Students will be assessed on:

1. **Basic Implementation (25%)**: Successful implementation of basic voice command processing
2. **Context Awareness (25%)**: Proper integration of environmental context
3. **LLM Integration (25%)**: Effective use of LLM for command interpretation
4. **Advanced Features (25%)**: Implementation of memory, learning, and adaptation

### Code Quality

- Proper ROS 2 node structure and lifecycle management
- Appropriate error handling and logging
- Clean, well-documented code
- Efficient resource usage

### Functionality

- Correct voice command recognition and execution
- Proper handling of environmental context
- Safe and robust operation
- Effective error recovery

## Resources

- [OpenAI Whisper Documentation](https://github.com/openai/whisper)
- [ROS 2 Documentation](https://docs.ros.org/)
- [Large Language Model Integration Guide](https://huggingface.co/docs/transformers/index)
- [Isaac Sim Integration Guide](https://docs.omniverse.nvidia.com/isaacsim/latest/index.html)

## Troubleshooting

### Common Issues

1. **Audio Device Not Found**: Ensure microphone permissions and device availability
2. **Whisper Model Loading**: Verify internet connection and model download
3. **ROS 2 Topic Connection**: Check that all nodes are on the same ROS domain
4. **LLM API Issues**: Verify API keys and service availability

### Performance Optimization

- Use smaller Whisper models for real-time applications
- Implement audio buffering for smoother processing
- Optimize LLM prompts for faster responses
- Use caching for repeated commands

## Next Steps

After completing these exercises, students should be prepared to:

1. Implement VLA systems for real robots
2. Integrate with more complex robotic platforms
3. Develop specialized applications for specific domains
4. Explore advanced topics like multimodal learning